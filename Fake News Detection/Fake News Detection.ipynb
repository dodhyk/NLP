{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316f4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83db2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739682a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake['fake'] = 1\n",
    "true['fake'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a783cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([fake.head(3000), true.head(3000)])\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdbbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_punctuation(sentence):\n",
    "    rem_punctuation = [word for word in sentence if word not in string.punctuation]\n",
    "    rem_punctuation = ''.join(rem_punctuation)\n",
    "    \n",
    "    return rem_punctuation\n",
    "\n",
    "df['text'] = df['text'].apply(del_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed8e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stopwords(sentence):\n",
    "    rem_stopword = [word for word in sentence.split() if word not in stopwords.words('english')]\n",
    "    rem_stopword = ' '.join(rem_stopword)\n",
    "    \n",
    "    return rem_stopword\n",
    "\n",
    "df['text'] = df['text'].apply(del_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4df4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(sentence):\n",
    "    lemma = [lemmatizer.lemmatize(word) for word in sentence.split()]\n",
    "    lemma = ' '.join(lemma)\n",
    "    \n",
    "    return lemma\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15490827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    sentences = sentences.lower()\n",
    "    sentences = sentences.apply(del_punctuation)\n",
    "    sentences = sentences.apply(del_stopwords)\n",
    "    sentences = sentences.apply(lemmatizing)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1bf644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df['text']\n",
    "y = df['fake']\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba43f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vector = vectorizer.fit_transform(X_train).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ee274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vector = vectorizer.transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "691b59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vector, y_train)\n",
    "y_result = model.predict(x_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71cdace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       885\n",
      "           1       1.00      0.99      1.00       915\n",
      "\n",
      "    accuracy                           1.00      1800\n",
      "   macro avg       1.00      1.00      1.00      1800\n",
      "weighted avg       1.00      1.00      1.00      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check The Accuracy\n",
    "print(classification_report(y_test, y_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "339c617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Test The Model\n",
    "sentences = '''One of Donald Trump s favorite punching bags is CNN. He even once tweeted a GIF image of himself punching a person with a CNN logo superimposed over the head \n",
    "            indicating that he d like to enact violence against CNN s reporters. Then there was the time he tweeted the  Trump Train roaring over  CNN.  Now, he s back at it \n",
    "            this time suggesting that  fake  CNN should be the ones representing America to the world, and that they are doing a bad job. Here is that tweet:.@FoxNews \n",
    "            is MUCH more important in the United States than CNN, but outside of the U.S., CNN International is still a major source of (Fake) news, and they represent\n",
    "            our Nation to the WORLD very poorly. The outside world does not see the truth from them!  Donald J. Trump (@realDonaldTrump) November 25, 2017Of course, \n",
    "            it is beneath the dignity of most people to respond to a moronic buffoon like Trump under normal circumstances. However, he is currently squatting in the \n",
    "            White House, and has his tiny orange hands on the levers of power   not to mention the nuclear codes   so they have to stoop to a Trumpian level when \n",
    "            personally attacked. However, being, well, you know, FIT to be doing the job they are doing, the good folks at CNN Communications fired back at Trump, and \n",
    "            their response is nothing short of perfect:It's not CNN's job to represent the U.S to the world. That's yours. Our job is to report the news. #FactsFirst   \n",
    "            CNN Communications (@CNNPR) November 25, 2017BOOM! Couldn t have asked for a sicker burn than this. And they are right of course   especially the part about\n",
    "            #FactsFirst. Trump has a problem with the truth, as we all well know. That s what makes what the CNN Communications people replied so fabulous. \n",
    "            It is the ultimate truth   something the likes of the pathological orange liar that is Donald Trump knows nothing about.Featured image via  \n",
    "            Andrew Burton/Getty Images '''\n",
    "sentences = del_punctuation(sentences)\n",
    "sentences = del_stopwords(sentences)\n",
    "sentences = lemmatizing(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "846b1b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Literally Fake News\n"
     ]
    }
   ],
   "source": [
    "sentences = vectorizer.transform([sentences])\n",
    "if model.predict(sentences)[0] == 1:\n",
    "    print(\"Literally Fake News\")\n",
    "else:\n",
    "    print(\"This is Real News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52ce58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
